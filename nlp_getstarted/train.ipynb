{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-04T05:59:20.442823Z",
     "start_time": "2025-04-04T05:59:20.394790Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df=pd.read_csv('data/train.csv')\n",
    "test_df=pd.read_csv('data/test.csv')\n",
    "print(train_df.head())\n",
    "train_df['text']=train_df['text'].str.lower()\n",
    "train_df['text']=train_df['text'].str.replace(r'[^\\w\\s]','')\n",
    "train_texts,val_texts,train_labels,val_labels=train_test_split(train_df['text'],train_df['target'],test_size=0.2,random_state=42)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T05:59:34.311284Z",
     "start_time": "2025-04-04T05:59:20.457823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# 加载 RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# 对文本进行 tokenization 和 padding\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts.tolist(), padding=True, truncation=True, max_length=128)\n",
    "\n",
    "train_encodings = tokenize_function(train_texts)\n",
    "val_encodings = tokenize_function(val_texts)\n",
    "\n",
    "# 转换为 torch tensors\n",
    "import torch\n",
    "class DisasterTweetsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 创建训练和验证数据集\n",
    "train_dataset = DisasterTweetsDataset(train_encodings, train_labels)\n",
    "val_dataset = DisasterTweetsDataset(val_encodings, val_labels)\n"
   ],
   "id": "36b36d38676b6ac7",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:07:49.040109Z",
     "start_time": "2025-04-04T06:07:49.033601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaModel\n",
    "\n",
    "class RoBERTaLSTMModel(nn.Module):\n",
    "    def __init__(self, roberta_model_name='roberta-base', hidden_dim=128, lstm_layers=2, num_labels=2):\n",
    "        super(RoBERTaLSTMModel, self).__init__()\n",
    "\n",
    "        # 加载预训练的 RoBERTa 模型\n",
    "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "\n",
    "        # LSTM 层\n",
    "        self.lstm = nn.LSTM(input_size=self.roberta.config.hidden_size,  # RoBERTa 输出的隐藏层维度\n",
    "                            hidden_size=hidden_dim,  # LSTM 的隐藏层维度\n",
    "                            num_layers=lstm_layers,  # LSTM 层数\n",
    "                            batch_first=True,  # batch size 在第一个维度\n",
    "                            bidirectional=True)  # 双向 LSTM\n",
    "\n",
    "        # 全连接层用于最终分类\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_labels)  # 双向 LSTM 输出维度是 hidden_dim * 2\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        # 通过 RoBERTa 获得序列的表示\n",
    "        roberta_output = self.roberta(input_ids=input_ids,\n",
    "                                      attention_mask=attention_mask,\n",
    "                                      token_type_ids=token_type_ids)\n",
    "\n",
    "        # 获取 RoBERTa 的隐藏层输出 (last_hidden_state)\n",
    "        hidden_states = roberta_output.last_hidden_state\n",
    "\n",
    "        # 通过 LSTM 层进行处理\n",
    "        lstm_output, (hn, cn) = self.lstm(hidden_states)  # LSTM 输出，hn 是最后一层的隐藏状态\n",
    "\n",
    "        # 使用 LSTM 最后的隐藏状态作为特征\n",
    "        # 双向LSTM，合并两端的隐藏状态\n",
    "        lstm_output = torch.cat((hn[-2, :, :], hn[-1, :, :]), dim=-1)\n",
    "\n",
    "        # 通过全连接层进行分类\n",
    "        logits = self.fc(lstm_output)\n",
    "\n",
    "        # 确保 logits 是二维张量 (batch_size, num_labels)\n",
    "        logits = logits.view(-1, self.fc.out_features)\n",
    "\n",
    "        return logits\n"
   ],
   "id": "f3973d4f7c8e1135",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:08:58.662002Z",
     "start_time": "2025-04-04T06:08:56.698802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "training_args = TrainingArguments(\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type='cosine',\n",
    "    output_dir='./result',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    ")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    preds = np.argmax(predictions, axis=1)  # 获取预测标签\n",
    "    f1 = f1_score(labels, preds)\n",
    "    return {\"f1\": f1}\n",
    "trainer = Trainer(\n",
    "    model=RoBERTaLSTMModel(),\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(training_args.device)  # 输出: cuda 代表使用的是 GPU"
   ],
   "id": "25400bde1fcda179",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T05:59:35.301825300Z",
     "start_time": "2025-04-04T05:41:29.430035Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6144161554aa7250",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:09:02.283790Z",
     "start_time": "2025-04-04T06:09:01.974264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer.train()\n",
    "# 输出评估结果\n",
    "\n"
   ],
   "id": "bc678bb4ed8d6f7a",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# 输出评估结果\u001B[39;00m\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\nlp\\lib\\site-packages\\transformers\\trainer.py:2123\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   2121\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   2122\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2124\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2125\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2126\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2127\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2128\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\nlp\\lib\\site-packages\\transformers\\trainer.py:2481\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2475\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2476\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[0;32m   2477\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   2478\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[0;32m   2479\u001B[0m )\n\u001B[0;32m   2480\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[1;32m-> 2481\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2483\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   2484\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[0;32m   2485\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[0;32m   2486\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[0;32m   2487\u001B[0m ):\n\u001B[0;32m   2488\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[0;32m   2489\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\nlp\\lib\\site-packages\\transformers\\trainer.py:3612\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[1;34m(***failed resolving arguments***)\u001B[0m\n\u001B[0;32m   3610\u001B[0m         scaled_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m   3611\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 3612\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3613\u001B[0m     \u001B[38;5;66;03m# Finally we need to normalize the loss for reporting\u001B[39;00m\n\u001B[0;32m   3614\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m num_items_in_batch \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\nlp\\lib\\site-packages\\accelerate\\accelerator.py:2246\u001B[0m, in \u001B[0;36mAccelerator.backward\u001B[1;34m(self, loss, **kwargs)\u001B[0m\n\u001B[0;32m   2244\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlomo_backward(loss, learning_rate)\n\u001B[0;32m   2245\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2246\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\nlp\\lib\\site-packages\\torch\\_tensor.py:521\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    513\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    514\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    519\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    520\u001B[0m     )\n\u001B[1;32m--> 521\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\nlp\\lib\\site-packages\\torch\\autograd\\__init__.py:282\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    273\u001B[0m inputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    274\u001B[0m     (inputs,)\n\u001B[0;32m    275\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(inputs, (torch\u001B[38;5;241m.\u001B[39mTensor, graph\u001B[38;5;241m.\u001B[39mGradientEdge))\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    278\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m()\n\u001B[0;32m    279\u001B[0m )\n\u001B[0;32m    281\u001B[0m grad_tensors_ \u001B[38;5;241m=\u001B[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001B[38;5;28mlen\u001B[39m(tensors))\n\u001B[1;32m--> 282\u001B[0m grad_tensors_ \u001B[38;5;241m=\u001B[39m \u001B[43m_make_grads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_grads_batched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retain_graph \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    284\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\nlp\\lib\\site-packages\\torch\\autograd\\__init__.py:151\u001B[0m, in \u001B[0;36m_make_grads\u001B[1;34m(outputs, grads, is_grads_batched)\u001B[0m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m out\u001B[38;5;241m.\u001B[39mrequires_grad:\n\u001B[0;32m    150\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m out\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 151\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    152\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad can be implicitly created only for scalar outputs\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    153\u001B[0m         )\n\u001B[0;32m    154\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m out\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mis_floating_point:\n\u001B[0;32m    155\u001B[0m         msg \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    156\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad can be implicitly created only for real scalar outputs\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    157\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mout\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    158\u001B[0m         )\n",
      "\u001B[1;31mRuntimeError\u001B[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T05:59:35.303867900Z",
     "start_time": "2025-04-04T05:47:51.507271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 在验证集上评估模型\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation Results: {eval_results}\")\n"
   ],
   "id": "8ce68dbc37bfda22",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3887951374053955, 'eval_f1': 0.8023648648648649, 'eval_runtime': 5.2753, 'eval_samples_per_second': 288.703, 'eval_steps_per_second': 36.206, 'epoch': 5.0}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T05:59:35.309106800Z",
     "start_time": "2025-04-04T05:47:56.817009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 对测试集进行预测\n",
    "test_encodings = tokenizer(list(test_df['text']), truncation=True, padding=True, max_length=128)\n",
    "test_dataset = DisasterTweetsDataset(test_encodings, [0] * len(test_df))  # 这里标签设为0，实际会被预测\n",
    "\n",
    "# 使用模型生成预测\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# 获取预测结果\n",
    "predicted_labels = predictions.predictions.argmax(axis=-1)\n",
    "\n",
    "# 创建提交文件\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'target': predicted_labels})\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ],
   "id": "b57713931bcb667",
   "outputs": [],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
